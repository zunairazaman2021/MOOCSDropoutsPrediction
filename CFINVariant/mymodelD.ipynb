{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zunai\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from time import time\n",
    "#from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFIN():\n",
    "    def __init__(self, a_feat_size, u_feat_size, c_feat_size, a_field_size, u_field_size, c_field_size,\n",
    "                 embedding_size=8,\n",
    "                 conv_size = 32,\n",
    "                 context_size = 16,\n",
    "                 deep_layers=[32, 32], dropout_deep=[0.5, 0.5, 0.5], dropout_attn = [0.5, 0.5],\n",
    "                 activation=tf.nn.relu,\n",
    "                 attn_size = 16,\n",
    "                 epoch=10, batch_size=256,\n",
    "                 \n",
    "                 learning_rate=0.001, optimizer_type=\"adam\",\n",
    "                 batch_norm=0, batch_norm_decay=0.995,\n",
    "                 verbose=False, random_seed=2016,\n",
    "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
    "                 l2_reg=0.0, attn=True):\n",
    "\n",
    "        self.a_feat_size = a_feat_size      \n",
    "        self.u_feat_size = u_feat_size\n",
    "        self.c_feat_size = c_feat_size\n",
    "        \n",
    "        self.a_field_size = a_field_size\n",
    "        self.u_field_size = u_field_size\n",
    "        self.c_field_size = c_field_size\n",
    "        self.conv_size = conv_size\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "         \n",
    "        self.deep_layers = deep_layers\n",
    "        self.dropout_deep = dropout_deep\n",
    "        self.dropout_attn = dropout_attn\n",
    "        self.activation = activation\n",
    "        self.l2_reg = l2_reg\n",
    "        self.attn_enable = attn\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.attn_size = attn_size\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "        self.greater_is_better = True \n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.train_result, self.valid_result = [], []\n",
    "        self._init_graph()\n",
    "        \n",
    "\n",
    "    def _init_graph(self):\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "            #tf.random.set_seed(self.random_seed)\n",
    "            \n",
    "            self.u_feat_index = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                                                 name=\"u_feat_index\")  # None * F\n",
    "            self.c_feat_index = tf.placeholder(tf.int32, shape=[None, None], name='c_feat_index')\n",
    "            self.a_feat_index = tf.placeholder(tf.int32, shape=[None, None], name='a_feat_index')\n",
    "            self.dropout_keep_attn = tf.placeholder(tf.float32, shape=[None], name = 'dropout_attn_uc')\n",
    "            \n",
    "            self.u_feat_value = tf.placeholder(tf.float32, shape=[None, None], name=\"u_feat_value\")\n",
    "            self.c_feat_value = tf.placeholder(tf.float32, shape=[None, None], name=\"c_feat_value\")\n",
    "            self.a_feat_value = tf.placeholder(tf.float32, shape=[None, None], name=\"a_feat_value\")\n",
    "            \n",
    "            self.label = tf.placeholder(tf.float32, shape=[None, 1], name=\"label\")  # None * 1\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_deep\")\n",
    "            self.train_phase = tf.placeholder(tf.bool, name=\"train_phase\")\n",
    "            \n",
    "            self.weights = self._initialize_weights()\n",
    "             \n",
    "            # model\n",
    "            self.a_embeddings = tf.nn.embedding_lookup(self.weights[\"a_feat_embeddings\"], self.a_feat_index)\n",
    "            self.u_embeddings = tf.nn.embedding_lookup(self.weights['u_feat_embeddings'], self.u_feat_index)\n",
    "            self.c_embeddings = tf.nn.embedding_lookup(self.weights['c_feat_embeddings'], self.c_feat_index)\n",
    "                       \n",
    "            u_feat_value = tf.reshape(self.u_feat_value, shape=[-1, self.u_field_size, 1])\n",
    "            c_feat_value = tf.reshape(self.c_feat_value, shape=[-1, self.c_field_size, 1])\n",
    "            a_feat_value = tf.reshape(self.a_feat_value, shape=[-1, self.a_field_size, 1])\n",
    "            self.c_embeddings = tf.multiply(self.c_embeddings, c_feat_value)\n",
    "            self.u_embeddings = tf.multiply(self.u_embeddings, u_feat_value)\n",
    "            self.a_embeddings = tf.multiply(self.a_embeddings, a_feat_value)\n",
    "            #if self.batch_norm:\n",
    "            #    self.a_embeddings = self.batch_norm_layer(self.a_embeddings, train_phase=self.train_phase, scope_bn='bn_conv')\n",
    "            self.a_embeddings = tf.nn.conv1d(self.a_embeddings, self.weights['a_conv_filter'], stride=5, padding='VALID',\n",
    "                                             data_format='NWC') + self.weights['a_conv_bias']\n",
    "          \n",
    "            self.a_embeddings = tf.nn.relu(self.a_embeddings)\n",
    "            \n",
    "            ##Change uc to separate user and course\n",
    "            #self.uc_embeddings = tf.concat([self.u_embeddings, self.c_embeddings], axis=1)\n",
    "            \n",
    "            #self.uc_inter = tf.nn.relu(tf.matmul(tf.reshape(self.uc_embeddings, shape=[-1, \n",
    "                                          #      (self.u_field_size+self.c_field_size)*self.embedding_size]), \n",
    "                                           #      self.weights['ctx_pool_weight'])+self.weights['ctx_pool_bias'])\n",
    "                    \n",
    "            #############################################\n",
    "            ############user model###########\n",
    "                   \n",
    "            self.uc_inter = tf.nn.relu(tf.matmul(tf.reshape(self.u_embeddings, shape=[-1, (self.u_field_size)*self.embedding_size]), \n",
    "                                                 self.weights['ctx_pool_weight']) + self.weights['ctx_pool_bias'])        \n",
    "\n",
    "            self.uca_inter = tf.concat([tf.tile(tf.expand_dims(self.uc_inter, 1), [1,self.a_field_size//5,1]), self.a_embeddings], 2)\n",
    "            \n",
    "            self.attn_logit = tf.nn.relu(tf.matmul(tf.reshape(self.uca_inter,  shape=[-1, self.conv_size + self.context_size]), self.weights['attn_out_1']) + self.weights['attn_bias_1'])\n",
    "            self.attn_w = tf.nn.softmax(tf.reshape(tf.matmul(self.attn_logit, self.weights['attn_out']), shape=[-1, self.a_field_size//5]))\n",
    "            if self.attn_enable:\n",
    "                self.a_weight_emb = tf.multiply(tf.expand_dims(self.attn_w,2), self.a_embeddings)\n",
    "            else:\n",
    "                self.a_weight_emb = self.a_embeddings\n",
    "\n",
    "            #############################################\n",
    "           \n",
    "            \n",
    "            \n",
    "            #############################################\n",
    "            ############course model###########\n",
    "            self.c_inter = tf.nn.relu(tf.matmul(tf.reshape(self.c_embeddings, shape=[-1, \n",
    "                                                (5)*self.embedding_size]), \n",
    "                                                 self.weights['course_pool_weight'])+self.weights['course_pool_bias'])        \n",
    "            \n",
    "            self.cca_inter = tf.concat([tf.tile(tf.expand_dims(self.c_inter, 1), [1,self.a_field_size//5,1]), \n",
    "                                        self.a_embeddings], 2)\n",
    "            \n",
    "            self.attn_logit_c = tf.nn.relu(tf.matmul(tf.reshape(self.cca_inter, \n",
    "                                                              shape=[-1, self.conv_size + self.context_size]), \n",
    "                                                   self.weights['attn_out_1']) + self.weights['attn_bias_1'])\n",
    "            self.attn_w_c = tf.nn.softmax(tf.reshape(tf.matmul(self.attn_logit_c, self.weights['attn_out']),\n",
    "                                                   shape=[-1, self.a_field_size//5]))\n",
    "            if self.attn_enable:\n",
    "                self.a_weight_emb = tf.multiply(tf.expand_dims(self.attn_w_c,2), self.a_embeddings)\n",
    "            else:\n",
    "                self.a_weight_emb = self.a_embeddings\n",
    "                \n",
    "            #############################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            deep_input = tf.reduce_sum(self.a_weight_emb, axis=1)\n",
    "            deep_input = tf.concat([deep_input, self.uc_inter], axis=1) \n",
    "            deep_input = tf.concat([deep_input, self.c_inter], axis=1) \n",
    "            #self.y_deep = tf.reshape(deep_input, shape=[-1,  self.conv_size])\n",
    "            self.y_deep = deep_input\n",
    "            self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[0])\n",
    "            for i in range(0, len(self.deep_layers)):\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep, self.weights[\"layer_%d\" %i]), self.weights[\"bias_%d\"%i])\n",
    "            #    if self.batch_norm:\n",
    "             #       self.y_deep = self.batch_norm_layer(self.y_deep, train_phase=self.train_phase, scope_bn=\"bn_%d\" %i) \n",
    "                self.y_deep = self.activation(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[1+i]) # dropout at each Deep layer\n",
    "            \n",
    "            self.out = tf.add(tf.matmul(self.y_deep, self.weights[\"logistic_weight\"]), self.weights[\"logistic_bias\"])\n",
    "\n",
    "            # loss\n",
    "            self.out = tf.nn.sigmoid(self.out)\n",
    "            self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            # l2 regularization on weights\n",
    "            # optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,\n",
    "                                                    epsilon=1e-8).minimize(self.loss)\n",
    "            self.saver = tf.train.Saver()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = self._init_session()\n",
    "            self.sess.run(init)\n",
    "\n",
    "            total_parameters = 0\n",
    "            for variable in self.weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            if self.verbose > 0:\n",
    "                print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "\n",
    "    def _init_session(self):\n",
    "        config = tf.ConfigProto()\n",
    "        config.allow_soft_placement = True\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "        return tf.Session(config=config)\n",
    "\n",
    "    def _init_layer_weight(self, input_size, output_size):\n",
    "        glorot = np.sqrt(2.0 / (input_size + output_size))\n",
    "        \n",
    "        return np.random.normal(loc=0, scale=glorot, size=(input_size, output_size)), np.random.normal(loc=0, scale=glorot, size=(1, output_size))\n",
    " \n",
    "    def _initialize_weights(self, c_node_embedding=None, u_node_embedding=None):\n",
    "        weights = dict()\n",
    "\n",
    "        # embeddings\n",
    "        weights[\"a_feat_embeddings\"] = tf.Variable(\n",
    "            tf.random_normal([self.a_feat_size, self.embedding_size], 0.0, 0.1),\n",
    "            name=\"a_feature_embeddings\")  # feature_size * K\n",
    "\n",
    "        weights[\"u_feat_embeddings\"] = tf.Variable(\n",
    "            tf.random_normal([self.u_feat_size, self.embedding_size], 0.0, 0.1),\n",
    "            name=\"u_feature_embeddings\")  # feature_size * K\n",
    "        \n",
    "        weights[\"c_feat_embeddings\"] = tf.Variable(\n",
    "            tf.random_normal([self.c_feat_size, self.embedding_size], 0.0, 0.1),\n",
    "            name=\"c_feature_embeddings\")  # feature_size * K\n",
    "         \n",
    "        #u_pool_w, u_pool_b = self._init_layer_weight((self.u_field_size+self.c_field_size)*self.embedding_size, self.context_size)\n",
    "        \n",
    "        #weights['ctx_pool_weight'] = tf.Variable(u_pool_w, name='ctx_pool_weight', dtype=np.float32)\n",
    "        \n",
    "        #weights[\"ctx_pool_bias\"] = tf.Variable(u_pool_b, name='ctx_pool_bias',dtype=np.float32)  # 1 * layers[0]\n",
    "        #####separate user\n",
    "        u_pool_w, u_pool_b = self._init_layer_weight((self.u_field_size)*self.embedding_size, self.context_size)\n",
    "        weights['ctx_pool_weight'] = tf.Variable(u_pool_w, name='ctx_pool_weight', dtype=np.float32)\n",
    "        weights[\"ctx_pool_bias\"] = tf.Variable(u_pool_b, name='ctx_pool_bias',dtype=np.float32)  # 1 * layers[0]\n",
    "        \n",
    "        \n",
    "        ######create separate course pool as well\n",
    "        cc_pool_w, cc_pool_b = self._init_layer_weight((self.c_field_size)*self.embedding_size, self.context_size)\n",
    "        weights['course_pool_weight'] = tf.Variable(cc_pool_w, name='course_pool_weight', dtype=np.float32)\n",
    "        weights[\"course_pool_bias\"] = tf.Variable(cc_pool_b, name='course_pool_bias',dtype=np.float32)  # 1 * layers[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        a_conv_w, a_conv_b = self._init_layer_weight(5*self.embedding_size, self.conv_size)\n",
    "        weights['a_conv_filter'] = tf.Variable(np.reshape(a_conv_w, [5, self.embedding_size, self.conv_size]), name='a_conv_filter', dtype=np.float32)\n",
    "        weights['a_conv_bias'] = tf.Variable(np.reshape(a_conv_b, [1,1,self.conv_size]), name='a_conv_bias', dtype=np.float32)\n",
    "        attn_out_1, attn_bias_1 = self._init_layer_weight(self.conv_size+self.context_size, self.attn_size)\n",
    "        weights['attn_out_1'] = tf.Variable(attn_out_1, name='attn_out_1', dtype='float32')\n",
    "        weights['attn_bias_1'] = tf.Variable(attn_bias_1, name='attn_bias_1', dtype='float32')\n",
    "        attn_out, attn_bias = self._init_layer_weight(self.attn_size, 1)\n",
    "        weights['attn_out'] = tf.Variable(attn_out, name='attn_out', dtype='float32')\n",
    "        \n",
    "        num_layer = len(self.deep_layers)\n",
    "        input_size = self.conv_size + self.context_size\n",
    "        \n",
    "        glorot = np.sqrt(2.0 / (input_size + self.deep_layers[0]))\n",
    "        weights[\"layer_0\"] = tf.Variable(\n",
    "            np.random.normal(loc=0, scale=glorot, size=(input_size, self.deep_layers[0])), dtype=np.float32)\n",
    "        weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[0])), dtype=np.float32)  # 1 * layers[0]\n",
    "        for i in range(1, num_layer):\n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[i-1] + self.deep_layers[i]))\n",
    "            weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i-1], self.deep_layers[i])),\n",
    "                dtype=np.float32)  # layers[i-1] * layers[i]\n",
    "            weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
    "                dtype=np.float32)  # 1 * layer[i]\n",
    "        \n",
    "        input_size = self.deep_layers[-1]\n",
    "        glorot = np.sqrt(2.0 / (input_size + 1))\n",
    "        weights[\"logistic_weight\"] = tf.Variable(\n",
    "                        np.random.normal(loc=0, scale=glorot, size=(input_size, 1)),\n",
    "                        dtype=np.float32)  # layers[i-1]*layers[i]\n",
    "        weights[\"logistic_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32)\n",
    "        return weights\n",
    "\n",
    "    def get_batch(self, ui, uv, ci, cv, ai, av, y, batch_size, index):\n",
    "        start = index * batch_size\n",
    "        end = (index+1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return ui[start:end], uv[start:end], ci[start:end], cv[start:end], ai[start:end], av[start:end], [[y_] for y_ in y[start:end]]\n",
    "\n",
    "    def shuffle_in_unison_scary(self, a, b, c, d,e,f,g):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(d)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(e)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(f)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(g)\n",
    "\n",
    "    def fit_on_batch(self, ui, uv, ci, cv, ai, av, y):\n",
    "        feed_dict = {self.u_feat_index: ui,\n",
    "                     self.u_feat_value: uv,\n",
    "                     self.c_feat_index: ci,\n",
    "                     self.c_feat_value: cv,\n",
    "                     self.a_feat_index: ai,\n",
    "                     self.a_feat_value: av,\n",
    "                     self.dropout_keep_deep: self.dropout_deep,\n",
    "                     self.dropout_keep_attn: self.dropout_attn,\n",
    "                     self.label: y,\n",
    "                     self.train_phase: True}\n",
    "        loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def fit(self, ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train,\n",
    "            ui_valid=None, uv_valid=None, ci_valid = None, cv_valid = None, ai_valid=None, av_valid=None,y_valid=None,\n",
    "            early_stopping = True, early_stopping_round = 5, max_epoch =None):\n",
    "        has_valid = uv_valid is not None\n",
    "        best_epoch = 0\n",
    "        if max_epoch:\n",
    "            self.epoch = max_epoch\n",
    "        for epoch in range(self.epoch):\n",
    "            t1 = time()\n",
    "            self.shuffle_in_unison_scary(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train)\n",
    "            total_batch = int(len(y_train) / self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch, y_batch = self.get_batch(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train, self.batch_size, i)\n",
    "                self.fit_on_batch(ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch, y_batch)\n",
    "\n",
    "            # evaluate training and validation datasets\n",
    "            train_result, train_deep, train_f1 = self.evaluate(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train)\n",
    "            self.train_result.append(train_result)\n",
    "            if has_valid:\n",
    "                valid_result, valid_deep, valid_f1= self.evaluate(ui_valid, uv_valid, ci_valid, cv_valid, ai_valid, av_valid, y_valid)\n",
    "                self.valid_result.append(valid_result)\n",
    "            if self.verbose > 0 and epoch % self.verbose == 0:\n",
    "                if has_valid:\n",
    "                    print(\"[%d] train-result=%.4f, valid-result=%.4f, valid-f1=%.4f [%.1f s]\"\n",
    "                        % (epoch + 1, train_result, valid_result, valid_f1, time() - t1))\n",
    "                else:\n",
    "                    print(\"[%d] train-result=%.4f [%.1f s]\"\n",
    "                        % (epoch + 1, train_result, time() - t1))\n",
    "            \n",
    "            if has_valid and early_stopping and self.training_termination(self.valid_result, early_stopping_round):\n",
    "                best_epoch = epoch - early_stopping_round + 1\n",
    "                print(\"best epoch: \", best_epoch)\n",
    "                return best_epoch\n",
    "                \n",
    "        \"\"\"         \n",
    "        if has_valid and refit:\n",
    "            if self.greater_is_better:\n",
    "                best_valid_score = max(self.valid_result)\n",
    "            else:\n",
    "                best_valid_score = min(self.valid_result)\n",
    "            best_epoch = self.valid_result.index(best_valid_score)\n",
    "            best_train_score = self.train_result[best_epoch]\n",
    "            ui_train = np.concatenate((ui_train, ui_valid), axis=0)\n",
    "            uv_train = np.concatenate((uv_train, uv_valid), axis=0)\n",
    "            ci_train = np.concatenate((ci_train, ci_valid), axis=0)\n",
    "            cv_train = np.concatenate((cv_train, cv_valid), axis=0)\n",
    "            ai_train = np.concatenate((ai_train, ai_valid), axis=0)\n",
    "            av_train = np.concatenate((av_train, av_valid), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    "\n",
    "            for epoch in range(100):\n",
    "                self.shuffle_in_unison_scary(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train)\n",
    "                total_batch = int(len(y_train) / self.batch_size)\n",
    "                for i in range(total_batch):\n",
    "                    ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch, y_batch = self.get_batch(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train, self.batch_size, i)\n",
    "                    self.fit_on_batch(ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch, y_batch)\n",
    "                train_result, train_deep, train_f1 = self.evaluate(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train)\n",
    "                if abs(train_result - best_train_score) < 0.001 or \\\n",
    "                    (self.greater_is_better and train_result > best_train_score) or \\\n",
    "                    ((not self.greater_is_better) and train_result < best_train_score):\n",
    "                    break\n",
    "        \"\"\"\n",
    "        save_path = self.saver.save(self.sess, \"my_model/CFIN\")\n",
    "        print(\"Save to path: \", save_path)\n",
    "        return save_path\n",
    "        \n",
    "    def training_termination(self, valid_result, early_stopping_round):\n",
    "        if len(valid_result) > early_stopping_round:\n",
    "            if self.greater_is_better:\n",
    "                if max(valid_result[-early_stopping_round:]) > valid_result[-1-early_stopping_round]:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "            else:\n",
    "                if min(valid_result[-early_stopping_round:]) < valid_result[-1-early_stopping_round]:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True    \n",
    "        return False\n",
    "\n",
    "    def get_feats(self, ui,uv,ci,cv, ai, av, fname):\n",
    "        dummy_y = [[1]] *len(ui)\n",
    "        feed_dict = {self.u_feat_index: ui,\n",
    "                     self.u_feat_value: uv,\n",
    "                     self.c_feat_index: ci,\n",
    "                     self.c_feat_value: cv,\n",
    "                     self.a_feat_index: ai,\n",
    "                     self.a_feat_value: av,\n",
    "                     self.dropout_keep_deep: [1.0] * len(self.dropout_deep),\n",
    "                     self.dropout_keep_attn: [1.0] * len(self.dropout_attn),\n",
    "                     self.label: dummy_y,\n",
    "                     self.train_phase: False}\n",
    "        \n",
    "        y_deep, y_rate= self.sess.run([self.y_deep, self.out], feed_dict = feed_dict)\n",
    "        return y_deep,y_rate\n",
    "    def get_attn(self, ui, uv, ci, cv, ai, av):\n",
    "        dummy_y = [1] * len(ui)\n",
    "        batch_index = 0\n",
    "        ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch ,y_batch = self.get_batch(ui, uv, ci, cv, ai, av, dummy_y, self.batch_size, batch_index)\n",
    "        attn_weight = None\n",
    "        uca_weight = None\n",
    "        while len(ui_batch) > 0:\n",
    "            num_batch = len(y_batch)\n",
    "            feed_dict = {self.u_feat_index: ui_batch,\n",
    "                         self.u_feat_value: uv_batch,\n",
    "                         self.c_feat_index: ci_batch,\n",
    "                         self.c_feat_value: cv_batch,\n",
    "                         self.a_feat_index: ai_batch,\n",
    "                         self.a_feat_value: av_batch,\n",
    "                         self.dropout_keep_deep: [1.0] * len(self.dropout_deep),\n",
    "                         self.dropout_keep_attn: [1.0] * len(self.dropout_attn),\n",
    "                         self.label: y_batch,\n",
    "                         self.train_phase: False}\n",
    "            attn, uca_inter = self.sess.run([self.attn_w,self.uca_inter], feed_dict=feed_dict)\n",
    "            if batch_index == 0:\n",
    "                attn_weight = np.reshape(attn, (num_batch,-1))\n",
    "                uca_weight = np.reshape(uca_inter, (num_batch, -1, self.embedding_size))\n",
    "            else:\n",
    "                attn_weight = np.concatenate((attn_weight, np.reshape(attn, (num_batch, -1))))\n",
    "                uca_weight = np.concatenate((uca_weight, np.reshape(uca_inter, (num_batch, -1, self.embedding_size))))\n",
    "            batch_index += 1\n",
    "            \n",
    "            ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch ,y_batch = self.get_batch(ui, uv, ci, cv, ai, av, dummy_y, self.batch_size, batch_index)\n",
    "        return attn_weight, uca_weight\n",
    "\n",
    "   \n",
    "    def predict(self, ui, uv, ci, cv, ai, av):\n",
    "        dummy_y = [1] * len(ui)\n",
    "        batch_index = 0\n",
    "        ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch ,y_batch = self.get_batch(ui, uv, ci, cv, ai, av, dummy_y, self.batch_size, batch_index)\n",
    "        y_pred = None\n",
    "        while len(ui_batch) > 0:\n",
    "            num_batch = len(y_batch)\n",
    "            feed_dict = {self.u_feat_index: ui_batch,\n",
    "                         self.u_feat_value: uv_batch,\n",
    "                         self.c_feat_index: ci_batch,\n",
    "                         self.c_feat_value: cv_batch,\n",
    "                         self.a_feat_index: ai_batch,\n",
    "                         self.a_feat_value: av_batch,\n",
    "                         self.dropout_keep_deep: [1.0] * len(self.dropout_deep),\n",
    "                         self.dropout_keep_attn: [1.0] * len(self.dropout_attn),\n",
    "                         self.label: y_batch,\n",
    "                         self.train_phase: False}\n",
    "            y_deep,batch_out = self.sess.run([self.y_deep,self.out], feed_dict=feed_dict)\n",
    "            if batch_index == 0:\n",
    "                y_pred = np.reshape(batch_out, (num_batch,))\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n",
    "\n",
    "            batch_index += 1\n",
    "            \n",
    "            ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch ,y_batch = self.get_batch(ui, uv, ci, cv, ai, av, dummy_y, self.batch_size, batch_index)\n",
    "        return y_deep,y_pred\n",
    "\n",
    "\n",
    "    def evaluate(self, ui, uv, ci, cv, ai, av, y):\n",
    "        y_deep, y_pred = self.predict(ui, uv, ci, cv, ai, av)  \n",
    "        return self.eval_metric(y, y_pred), y_deep, f1_score(y, [1 if x>0.5 else 0 for x in y_pred])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
